<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-10653444-5', 'auto');
  ga('send', 'pageview');

  </script>
  <link rel="stylesheet" href="./codes/my2.css">
  <link rel="stylesheet" href="./codes/mytheme2.css">
  <script src="./codes/my.js"></script>

  <title>Fereshteh Sadeghi</title>
  
  <meta name="author" content="Fereshteh Sadeghi">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/logo-uw-2011.jpg">

  <script type="text/javascript">
   function toggle_vis(id) {
       var e = document.getElementById(id);
       if (e.style.display == 'none')
           e.style.display = 'inline';
       else
           e.style.display = 'none';
   }
   </script>

</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/fsadeghi.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/fsadeghi.jpg" class="img-circle"></a>
              <br>
              <p style="text-align:center">
              <a href="./fsadeghi_photo.html" target="_blank">Photos</a>
            </p>
        </p>

            </td>

            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Fereshteh&nbsp; Sadeghi</name><br>
                fsadeghi&nbsp; at&nbsp; cs&nbsp; dot&nbsp; washington&nbsp; dot&nbsp; edu
              </p>

              <p style="text-align:center"><a href="https://twitter.com/fereshteh_sa?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false">Follow @fereshteh_sa</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>

            
              <p style="text-align:center">
                <a href="https://digital.lib.washington.edu/researchworks/bitstream/handle/1773/43660/Sadeghi_washington_0250E_19800.pdf?sequence=1&isAllowed=y">PhD Thesis</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=vS8b6GwAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="mailto:fsadeghi@cs.washington.edu">Email</a> &nbsp
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News!</heading><p></p>
              <ul style="list-style: none;">
                <li><span style="color:#FF0080"><font size="2" ><a href="https://arxiv.org/pdf/1908.08031.pdf" ><img class="img-circle" src="./paper_figs/mushr_small.png" alt="Generic placeholder image" style="width: 35px; height: 35px;"></a>&nbsp&nbsp~ Aug. 2019 ~</span> <a href="https://arxiv.org/pdf/1908.08031.pd"></a> Excited to release <a href="https://news.cs.washington.edu/2019/08/21/allen-school-releases-mushr-robotic-race-car-platform-to-drive-advances-in-ai-research-and-education/">MuSHR! </a> Also featured in <a href="https://www.geekwire.com/2019/robotic-race-car-platform-univ-washington-designed-speed-research-around/">GeekWire</a> and <a href="https://www.wired.com/story/small-cars-help-drive-autonomous-future/?mbid=social_tw_transport&utm_brand=wired&utm_campaign=wiredtransportation&utm_medium=social&utm_social-type=owned&utm_source=twitter"> WIRED</a>! </li><p></p>
                <li><span style="color:#FF0080"><font size="2" ><a href="https://youtu.be/UAKQhciYN6w " ><img class="img-circle" src="./paper_figs/rss2019.png" alt="Generic placeholder image" style="width: 35px; height: 35px;"></a>&nbsp&nbsp~ May. 2019 ~</span>  <a href="https://arxiv.org/pdf/1902.05947.pdf"></a> <a href="https://youtu.be/UAKQhciYN6w ">DIViS: Domain Invariant Visual Servoing </a> is accepted in <a href="http://www.roboticsconference.org/">RSS 2019 </a>!</li><p></p>
                <li><span style="color:#FF0080"><font size="2" ><a href="https://digital.lib.washington.edu/researchworks/bitstream/handle/1773/43660/Sadeghi_washington_0250E_19800.pdf?sequence=1&isAllowed=y" ><img class="img-circle" src="./paper_figs/huskey_grad.png" alt="Generic placeholder image" style="width: 35px; height: 35px;"></a>&nbsp&nbsp~ Feb. 2019 ~</span> Successfully defended my PhD thesis: <a href="https://digital.lib.washington.edu/researchworks/handle/1773/43660">"Domain Invariant and Semantic Aware Visual Servoing" </a>!</li><p></p>
                <li> <span style="color:#FF0080"><font size="2" ><img class="img-circle" src="./paper_figs/nvidia.jpg" alt="Generic placeholder image" style="width: 35px; height: 35px;"> ~ May. 2017 ~ </span> Won the <a href="http://research.nvidia.com/grad_fellows/2017/FereshtehSadeghi">NVIDIA Graduate Fellowship 2017- 2018</a> award! </li>
              

                <p>
                <a href="javascript:toggle_vis('oldnews')"><b> >>> Older news !! ...>>></b></a>
                <div id="oldnews" style="display:none"> 

                <!-- <div id="oldr" style="display:none"> -->
                  <li> <span style="color:#009933"><font size="1" ><a href="https://youtu.be/UAKQhciYN6w " ><img class="img-circle" src="./paper_figs/thumbnail_DIViS.png" alt="Generic placeholder image" style="width: 25px; height: 25px;"></a> ~ Feb. 2019 ~ </span> New <a href="https://arxiv.org/pdf/1902.05947.pdf">arXiv</a> preprint <a href="https://youtu.be/UAKQhciYN6w ">DIViS: Domain Invariant Visual Servoing for Collision-Free Goal Reaching</a> posted! </font></li>
                  <li> <span style="color:#009933"><font size="1" ><a href="https://www.ri.cmu.edu/event/acquiring-and-transferring-generalizable-vision-based-robot-skills/" ><img class="img-circle" src="./paper_figs/RI_VASC.jpg" alt="Generic placeholder image" style="width: 25px; height: 25px;"></a> ~ Oct. 2018 ~ </span> I will be giving an invited talk at the <a href="https://www.ri.cmu.edu/event/acquiring-and-transferring-generalizable-vision-based-robot-skills/">RI VASC Seminar.</a></font></li>
                  <li> <span style="color:#009933"><font size="1" ><a href="https://ai.googleblog.com/2018/06/teaching-uncalibrated-robots-to_22.html" ><img class="img-circle" src="./paper_figs/GoogleAI.png" alt="Generic placeholder image" style="width: 25px; height: 25px;"></a> ~ Jun. 2018 ~ </span> I wrote a <a href="https://ai.googleblog.com/2018/06/teaching-uncalibrated-robots-to_22.html">GoogleAI </a> blog post :  <a href="https://ai.googleblog.com/2018/06/teaching-uncalibrated-robots-to_22.html">Teaching Uncalibrated Robots to Visually Self-Adapt</a>.</font></li>

                  <li> <span style="color:#009933"><font size="1" ><a href="https://sites.google.com/view/rss2018women/" ><img class="img-circle" src="./paper_figs/RSS18.png" alt="Generic placeholder image" style="width: 25px; height: 25px;"></a> ~ Jun. 2018 ~ </span> I am co-organizing the <a href="https://sites.google.com/view/rss2018women/">4th Women in Robotics Workshop </a> at <a href="http://www.roboticsconference.org/">Robotics: Science and Systems Conference(R:SS)</a>.</font></li>

                  <li> <span style="color:#009933"><font size="1" ><a href="https://fsadeghi.github.io/Sim2RealViewInvariantServo/" ><img class="img-circle" src="./paper_figs/cvpr18logo_3.jpg" alt="Generic placeholder image" style="width: 25px; height: 25px;"></a> ~ Feb. 2018 ~ </span> <a href="https://www.youtube.com/watch?v=oLgM2Bnb7fo&feature=youtu.be">Sim2Real View Invariant Visual Servoing </a> accepted to <a href="http://cvpr2018.thecvf.com/">CVPR2018</a>.</font></li>

                  <li> <span style="color:#009933"><font size="1" ><a href="https://fsadeghi.github.io/Sim2RealViewInvariantServo/" ><img class="img-circle" src="./paper_figs/sim2realviservo.jpg" alt="Generic placeholder image" style="width: 25px; height: 25px;"></a> ~ Dec. 2017 ~ </span> Sim2Real View Invariant Visual Servoing posted on <a href="https://arxiv.org/pdf/1712.07642.pdf">arxiv </a>. See the video <a href="https://www.youtube.com/watch?v=oLgM2Bnb7fo&feature=youtu.be">here </a>.</font></li>

                  <li> <span style="color:#009933"><font size="1" ><a href="https://www.iros2017.org/" ><img class="img-circle" src="./paper_figs/iros.jpg" alt="Generic placeholder image" style="width: 25px; height: 25px;"></a> ~ Sept. 2017 ~ </span> At IROS 2017, I will be giving an invited talk in the workshop on <a href="http://www.seas.upenn.edu/~loiannog/workshopIROS2017uav/index.html#program">Vision-based Agile Autonomous Navigation of UAVs </a>.</font></li>

                  <li> <span style="color:#009933"><font size="1" ><a href="https://research.google.com/teams/brain/" ><img class="img-circle" src="./paper_figs/brain.jpg" alt="Generic placeholder image" style="width: 25px; height: 25px;"></a> ~ June. 2017 ~ </span>I am spending time at  <a href="https://research.google.com/teams/brain/robotics/">Google Brain Robotics </a> for internship</li>
                  
                  <li> <span style="color:#009933"><font size="1" ><a href="http://www.roboticsconference.org/program/papers/13/" ><img class="img-circle" src="./paper_figs/RSS.png" alt="Generic placeholder image" style="width: 25px; height: 25px;"></a> ~ April. 2017 ~ </span> Our <a href="https://arxiv.org/pdf/1611.04201.pdf">CAD<sup>2</sup>RL paper</a> got accepted in <a href="http://www.roboticsconference.org/">Robotics: Science and Systems Conference(R:SS) !! </a> Check the <a href="https://fsadeghi.github.io/CAD2RL/">video</a> here. </li>
                  <li> <span style="color:#009933"><font size="1" > ~ Nov. 2016 ~ </span><a href="https://arxiv.org/pdf/1611.04201.pdf">New preprint</a> on deep reinforcement learning and deep robotic learning posted! Check the <a href="https://fsadeghi.github.io/CAD2RL/">video</a> here. </li>
                  <li> <span style="color:#009933"><font size="1" > ~ Nov. 2016 ~ </span> At <a href="http://nips.cc/">NIPS'16</a>, I will be giving a contributed talk in the <a href="https://sites.google.com/site/nips16interaction/">DLAI</a> workshop </li>
                  <li> <span style="color:#009933"><font size="1" > ~ Oct. 2016 ~ </span> I will be serving as a program committee member in <a href="http://cvpr2017.thecvf.com/">CVPR'17</a>.</li>
                  <li> <span style="color:#009933"><font size="1" > ~ Jun. 2016 ~ </span> Spending my summer as a research intern at <a href="https://www.magicleap.com/">Magic Leap</a> ! It feels magical to be a leaper :) <a href="https://www.magicleap.com/#/company" ><img class="img-circle" src="./paper_figs/leaper.png" alt="Generic placeholder image" style="width: 30px; height: 30px;"></a> </li>
                  <li> <span style="color:#009933" ><font size="1" > ~ Apr. 2016 ~ </span> I am guest lecturer at ML course (CSEP546) for Spring'16 quarter.</font></li>
                  <li> <span style="color:#009933"><font size="1" > ~ Feb. 2016 ~ </span> I will be serving as a program committee member in <a href="http://www.eccv2016.org/">ECCV'16</a>.</font></li>
                  <li> <span style="color:#009933"><font size="1" > ~ Feb. 2016 ~ </span> I am guest lecturer at AI course (CSEP573) for Winter'16 quarter.</font></li>
                  <li> <span style="color:#009933"><font size="1" > ~ Nov. 2015 ~ </span> I will be serving as a program committee member in <a href="http://www.pamitc.org/cvpr16/">CVPR'16</a>.</font></li>
                  <li> <span style="color:#009933"><font size="1" > ~ Oct. 2015 ~ </span> I am guest lecturer at AI course (CSE473) for Fall'15 quarter.</font></li>
                  <li> <span style="color:#009933"><font size="1" > ~ Sep. 2015 ~ </span>I am organizing <a href="https://courses.cs.washington.edu/courses/cse590v/15au/">Vision Seminar (CSE 590v)</a> for the Fall'15 quarter.</font></li>
                  <li> <span style="color:#009933"><font size="1" > ~ Sep. 2015 ~ </span> Our Visual analogy paper got accepted in <a href="https://nips.cc/Conferences/2015">NIPS'15</a>.</font></li>
                  <li> <span style="color:#009933"><font size="1" > ~ Sep. 2015 ~ </span> Our Visual entailment paper got accepted as oral in <a href="http://pamitc.org/iccv15/">ICCV'15</a>.</font></li>
                  <li> <span style="color:#009933"><font size="1" > ~ Mar. 2015 ~ </span> VisKE got accepted in <a href="http://www.pamitc.org/cvpr15/">CVPR'15</a>.</font></li>
                  <li> <span style="color:#009933"><font size="1" > ~ Jan. 2015 ~ </span> My album creation paper is featured in <a href="http://www.eurekalert.org/pub_releases/2015-01/dr-drc010715.php">EurekAlert</a>.</li>
                  <li> <span style="color:#009933"><font size="1" > ~ Dec. 2014 ~ </span> I will organize the GRAIL seminar (cse 591) for the Spring'15 quarter. </font></li>
                  <li> <span style="color:#009933"><font size="1" > ~ Nov. 2014 ~ </span> I will be serving as a program committee member in <a href="http://www.pamitc.org/cvpr15/">CVPR'15</a>.</font></li>
                </div> 
                </p>
              </ul>
            </td>
          </tr>
        </tbody></table>

        <hr class="featurette-divider">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Talks</heading>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <center>
                  <iframe width="373" height="210" src="https://www.youtube.com/embed/rovQlRFSawo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                  </center>
                  <p><font size="2" ><a href="http://www.roboticsproceedings.org/rss15/p55.pdf">Robotics: Science and Systems 2019</a> </font></p>
                </td>
                <td style="padding:2.5%;width:40%;vertical-align:middle">
                  <center>
                  <iframe width="373" height="210" src="https://www.youtube.com/embed/jT1p1HAiDCg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                  </center>
                  <p><font size="2" ><a href="https://arxiv.org/pdf/1712.07642.pdf">UC Berkeley BAIR/BDD Seminar Series</a> </font></p>
                </td>
              </tr>
            </td>
          </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <ul style="list-style: none;">
                <li> <span style="color:#FF0080"><font size="2" ><img class="img-circle" src="./paper_figs/mic_shadow.jpg" alt="Generic placeholder image" style="width: 35px; height: 35px;"> ~ June. 2019 ~ </span>  <a href="https://www.youtube.com/watch?v=rovQlRFSawo">Robotics: Science and Systems Conference 2019</a> </li>
                <li> <span style="color:#FF0080"><font size="2" ><img class="img-circle" src="./paper_figs/mic_shadow.jpg" alt="Generic placeholder image" style="width: 35px; height: 35px;"> ~ June. 2019 ~ </span>  <a href="https://sim2real.github.io/">Workshop on Closing the Reality Gap in Sim2real at RSS</a>  </li>

                <p>
                <a href="javascript:toggle_vis('moretalks')"><b> >>> Show more ...>>></b></a>
                <div id="moretalks" style="display:none"> 
                  <li> <span style="color:#FF0080"><font size="2" > ~ October. 2018 ~ </span>  <a href="https://www.ri.cmu.edu/event acquiring-and-transferring-generalizable-vision-based-robot-skills/">RI VASC Seminar</a> </li>
                  <li> <span style="color:#FF0080"><font size="2" > ~ April. 2018 ~ </span>  <a href="http://citris-uc.org/bair-seminar-series/">BAIR Seminar</a>, see the video <a href="https://www.youtube.com/watch?v=jT1p1HAiDCg">here </a></li> 
                  <li> <span style="color:#FF0080"><font size="2" > ~ March. 2018 ~ </span>  <a href="http://www.gputechconf.com/">GPU Technology Conference 2018</a>  </li>
                  <li> <span style="color:#FF0080"><font size="2" > ~ July. 2017 ~ </span>  <a href="http://www.roboticsproceedings.org/rss13/p34.pdf">Robotics: Science and Systems Conference 2017</a>  </li>
                  <li> <span style="color:#FF0080"><font size="2" > ~ May. 2017 ~ </span>  <a href="http://www.gputechconf.com/">GPU Technology Conference 2017</a>, see the video<a href="https://youtu.be/qFmH4oZPlYY"> here</a> </li>
                  <li> <span style="color:#FF0080"><font size="2" > ~ May. 2017 ~ </span>  <a href="http://sorl.citris-uc.org/">Symposium on Robot Learning 2017</a>  </li>
                  <li> <span style="color:#FF0080"><font size="2" > ~ April. 2017 ~ </span>  <a href="http://citris-uc.org/bair-seminar-series/">BAIR Seminar</a>  </li>
                  <li> <span style="color:#FF0080"><font size="2" > ~ Dec. 2016 ~ </span>  <a href="https://sites.google.com/site/nips16interaction/">Deep Learning for Action and Interaction, NIPS 2016</a>, see the video <a href="https://youtu.be/HpmA4t0NWks">here</a>  </li>
                </div>
                </p>
              </ul>
            </td>
            </tr>
        </tbody></table>

        <hr class="featurette-divider">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <heading>Work Experience</heading>
                <td style="padding:2.5%;width:25%;vertical-align:middle">
                  <a href="/paper_figs/mickey.jpg"><img style="width:100%;max-width:100%" alt="Disney Research" src="./paper_figs/mickey.jpg" class="img-circle"></a>
                  <center><p><strong><a href="https://www.disneyresearch.com/">Disney Research</a></strong></p></center><br>
                </td>
                <td style="padding:2.5%;width:25%;vertical-align:middle">
                  <a href="/paper_figs/magic_leap.png"><img style="width:100%;max-width:100%" alt="Magic Leap" src="./paper_figs/magic_leap.png" class="img-circle"></a>
                  <center><p><strong><a href="https://www.magicleap.com/">Magic Leap</a></strong></p></center><br>
                </td>
                <td style="padding:2.5%;width:25%;vertical-align:middle">
                  <a href="/paper_figs/mickey.jpg"><img style="width:100%;max-width:100%" alt="Google Brain" src="./paper_figs/google_brain_logo.png" class="img-circle"></a>
                  <center><p><strong><a href="https://research.google/teams/brain/robotics/">Google Brain Robotics</a></strong></p></center><br>
                </td>
                <td style="padding:2.5%;width:25%;vertical-align:middle">
                  <a href="/paper_figs/UW.png"><img style="width:100%;max-width:100%" alt="University of Washington" src="./paper_figs/cse_logo.png" class="img-circle"></a>
                  <center><p><strong><a href="https://www.engr.washington.edu/about/bldgs/cse">University of Washington</a></strong></p></center><br>
                </td>

            </tr>
        </tbody></table>

        <hr class="featurette-divider">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <heading>Videos</heading>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <center>
                  <iframe width="373" height="210" src="https://www.youtube.com/embed/UAKQhciYN6w" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                  </center>
                  <p><font size="2" ><a href="http://www.roboticsproceedings.org/rss15/p55.pdf">DIViS: Domain Invariant Visual Servoing</a> </font></p><br>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <center>
                  <iframe width="373" height="210" src="https://www.youtube.com/embed/oLgM2Bnb7fo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                  </center>
                  <p><font size="2" ><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Sadeghi_Sim2Real_Viewpoint_Invariant_CVPR_2018_paper.pdf">Sim2Real Viewpoint Invariant Visual Servoing by Recurrent Control</a></font></p>
                </td>
              
              <tr style="padding:0px">
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <center>
                  <iframe width="373" height="210" src="https://www.youtube.com/embed/nXBWmzFrj5s" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                  </center>
                  <p><font size="2" ><a href="http://www.roboticsproceedings.org/rss13/p34.pdf">CAD2RL, Simulation Randomization &nbsp;&nbsp;&nbsp;&nbsp;(a.k.a. Domain Randomization)</a> </font></p>
                </td>
              
          </tr>
        </tbody></table>

        <hr class="featurette-divider">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in various topics in robotics, machine learning and computer vision. Here is the list of my selected publications. For the complete list of my publications please visit my <a target="_blank" href="http://scholar.google.com/citations?user=vS8b6GwAAAAJ&hl=en">Google Scholar </a> page and <a target="_blank" href="http://www.informatik.uni-trier.de/~ley/pers/hd/s/Sadeghi:Fereshteh.html">DBLP</a>.</p>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <subheading>PhD Thesis</subheading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src="./paper_figs/logo-uw-2011.png" alt="Thesis" width="160" style="border-style: none">
            </td>
            <td width="70%" valign="middle">
              <a href="https://digital.lib.washington.edu/researchworks/handle/1773/43660" id="MCG_journal">
              <papertitle>Domain Invariant and Semantic Aware Visual Servoing</papertitle>
              </a>
              <br>
              <strong>Fereshteh Sadeghi</strong>
              <br>
              <em>Ph.D. Dissertation, University of Washington, Computer Science</em>, 2019
              <br>
              <a href=""><metap>bibtex</metap></a> 
              <a href=""></a>
              <p></p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <subheading>Preprints</subheading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src="./paper_figs/mushr.jpg" alt="Thesis" width="190" style="border-style: none"><br><br><br><br><br>
            </td>
            <td width="70%" valign="middle">
              <a href="https://arxiv.org/pdf/1908.08031.pdf" id="MCG_journal">
              <papertitle>MuSHR: A Low-Cost, Open-Source Robotic Racecar for Education and Research</papertitle>
              </a>
              <br>
              <author>S. Srinivasa, P. Lancaster, J. Michalove, M. Schmittle, C. Summers, M. Rockett, J. R. Smith, S. Choudhury, C. Mavrogiannis,</author> <strong>Fereshteh Sadeghi</strong>
              <br>
              <em>arXiv preprint arXiv:1908.08031 </em>, 2019
              <br>
              <a href=""><metap>bibtex</metap></a> 
              <a href=""></a>
              <p></p>
              <p>MuSHR is a low-cost, open-source robotic racecar platform for education and research. MuSHR aspires to contribute towards democratizing the field of robotics as a low-cost platform that can be built and deployed by following detailed, open documentation and do-it-yourself tutorials.</p><br>
            </td>
          </tr>
        </tbody></table>

       

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <subheading>Peer Reviewed Articles</subheading>
            </td>
          </tr>
        </tbody></table>

        

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src="./paper_figs/teaser_DIViS.png" alt="DiVIS" width="190" style="border-style: none"><br><br><br>
            </td>
            <td width="70%" valign="middle">
              <a href="https://youtu.be/UAKQhciYN6w" id="MCG_journal">
              <papertitle>DIViS: Domain Invariant Visual Servoing for Collision-Free Goal Reaching</papertitle>
              </a>
              <br>
              <strong>Fereshteh Sadeghi</strong>
              <br>
              <em>Robotics: Science and Systems (RSS)</em>, 2019
              <br>
              <a href="https://fsadeghi.github.io/DIViS/"><metap>project page</metap></a> /
              <a href="https://www.youtube.com/watch?v=oLgM2Bnb7fo"><metap>video</metap></a> /
              <a href="https://www.youtube.com/watch?v=rovQlRFSawo"><metap>talk video</metap></a> /
              <a href=""><metap>bibtex</metap></a>
              <a href=""></a>
              <p></p>
              <p>In this paper, we investigate how to minimize human effort and intervention to teach robots perform real world tasks that incorporate semantics and we propose DIViS, a Domain Invariant policy learning approach for collision free Visual Servoing. While DIViS does not use any real robot data at the training time it is capable of servoing real mobile robots to semantic object categories in many diverse and unstructured real-world environments.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src="./paper_figs/sim2realviservo.png" alt="DiVIS" width="190" style="border-style: none"><br><br>
            </td>
            <td width="70%" valign="middle">
              <a href="https://www.youtube.com/watch?v=oLgM2Bnb7fo" id="MCG_journal">
              <papertitle>Sim2Real View Invariant Visual Servoing by Recurrent Control</papertitle>
              </a>
              <br>
              <strong>Fereshteh Sadeghi</strong><author>, Alexander Toshev, Eric Jang, Sergey Levine</author>
              <br>
              <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2018
              <br>
              <a href="https://fsadeghi.github.io/Sim2RealViewInvariantServo/"><metap>project page</metap></a> /
              <a href="https://www.youtube.com/watch?v=oLgM2Bnb7fo"><metap>video</metap></a> /
              <a href="https://www.youtube.com/watch?v=jT1p1HAiDCg"><metap>talk video</metap></a> /
              <a href="https://arxiv.org/pdf/1712.07642.pdf"><metap>arXiv</metap></a> /
              <a href=""><metap>bibtex</metap></a>
              <a href=""></a>
              <p></p>
              <p>In this paper, we study how viewpoint-independent visual servoing skills can be learned automatically in a robotic manipulation scenario. To this end, we train a deep, recurrent controller that can automatically determine which actions move the end-point of a robotic arm to a desired object. We show how we can learn this recurrent controller using simulated data, and then describe how the resulting model can be transferred to a real-world Kuka IIWA robotic arm.</p>
            </td>
          </tr>

          
          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src="./paper_figs/CAD2RL.png" alt="CAD2RL" width="190" style="border-style: none"><br><br><br><br><br><br><br>
            </td>
            <td width="70%" valign="middle">
              <a href="https://youtu.be/nXBWmzFrj5s" id="MCG_journal">
              <papertitle>CAD<sup>2</sup>RL : Real Single-Image Flight without a Single Real Image</papertitle>
              </a>
              <br>
              <strong>Fereshteh Sadeghi</strong><author>, Sergey Levine</author>
              <br>
              <em>Robotics: Science and Systems (RSS)</em>, 2017 <br>
              <em>Neural Information Processing Systems (NeurIPS)</em> DL for action and interaction workshop <span class="highlight">Spotlight</span>, 2016<br> 
              <em>Neural Information Processing Systems (NeurIPS)</em> DeepRL Workshop, 2016  
              <br>
              <a href="https://fsadeghi.github.io/CAD2RL/"><metap>project page</metap></a> /
              <a href="https://www.youtube.com/watch?v=nXBWmzFrj5s"><metap>video</metap></a> /
              <a href="https://youtu.be/qFmH4oZPlYY"><metap>talk video</metap></a> /
              <a href="https://arxiv.org/pdf/1611.04201.pdf"><metap>arXiv</metap></a> /
              <a href=""><metap>bibtex</metap></a>
              <a href=""></a>
              <p></p>
              <p>We propose simulation randomization (a.k.a. Domain Randomization) for direct transfer of vision-based policies from simulation to the real world. CAD<sup>2</sup>RL is a flight controller for Collision Avoidance via Deep Reinforcement Learning that can be used to perform collision-free flight in the real world although it is entirely trained in a 3D CAD model simulator. Our method uses only single RGB images from a monocular camera mounted on the robot as the input and is specialized for indoor hallway following and obstacle avoidance.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src="./paper_figs/Visalogy_teaser.png" alt="Visalogy" width="190" style="border-style: none"><br><br><br><br><br>
            </td>
            <td width="70%" valign="middle">
              <a href="http://arxiv.org/pdf/1510.08973.pdf" id="MCG_journal">
              <papertitle>VISALOGY: Answering Visual Analogy Questions</papertitle>
              </a>
              <br>
              <strong>Fereshteh Sadeghi</strong><author>, C. Lawrence Zitnick, Ali Farhadi</author>
              <br>
              <em>Neural Information Processing Systems (NeurIPS)</em>, 2015               
              <br>
              <a href="./papers/fsadeghi_nips_poster_visual_analogy.pdf"><metap>poster</metap></a> /
              <a href="./papers/fsadeghi_visual_analogy_slides.pdf"><metap>slides</metap></a> /
              <a href="http://arxiv.org/pdf/1510.08973.pdf"><metap>arXiv</metap></a> /
              <a href=""><metap>bibtex</metap></a>
              <a href=""></a>
              <p></p>
              <p>In this paper, we study the problem of answering visual analogy questions. These questions take the form of image A is to image B as image C is to what. Answering these questions entails discovering the mapping from image A to image B and then extending the mapping to image C and searching for the image D such that the relation from A to B holds for C to D. We pose this problem as learning an embedding that encourages pairs of analogous images with similar transformations to be close together using convolutional neural networks with a quadruple Siamese architecture.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src="./paper_figs/VisualEntailment_teaser.jpg" alt="SPT" width="190" style="border-style: none"><br><br><br><br><br><br><br>
            </td>
            <td width="70%" valign="middle">
              <a href="http://arxiv.org/pdf/1509.08075v1.pdf" id="MCG_journal">
              <papertitle>Segment-Phrase Table for Semantic Segmentation, Visual Entailment and Paraphrasing</papertitle>
              </a>
              <br>
              <author>Hamid Izadinia, </author><strong>Fereshteh Sadeghi</strong><author>, Santosh K. Divvala, Yejin Choi, Ali Farhadi</author>
              <br>
              <em>International Conference on Computer Vision (ICCV)</em>, <span class="highlight">Oral</span> 2015               
              <br>
              <a href="http://arxiv.org/pdf/1509.08075v1.pdf"><metap>arXiv</metap></a> /
              <a href=""><metap>bibtex</metap></a>
              <p></p>
              <p>Segment-Phrase Table (SPT) is a large collection of bijective associations between textual phrases and their corresponding segmentations. We show that fine-grained textual labels facilitate contextual reasoning that helps in satisfying semantic constraints across image segments. This feature enables us to achieve state-of-the-art segmentation results on benchmark datasets. We also show that the association of high-quality segmentations to textual phrases aids in richer semantic understanding and reasoning of these textual phrases which motivates the problem of visual entailment and visual paraphrasing.</p>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src="./paper_figs/VisKE_teasor.png" alt="VisKE" width="190" style="border-style: none"><br><br><br><br><br><br><br><br>
            </td>
            <td width="70%" valign="middle">
              <a href="/papers/fsadeghi_VisKE.pdf" id="MCG_journal">
              <papertitle>VisKE: Visual Knowledge Extraction and Question Answering by Visual Verification of Relation Phrases</papertitle>
              </a>
              <br>
              <strong>Fereshteh Sadeghi</strong><author>, Santosh K. Divvala, Ali Farhadi</author>
              <br>
              <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2015               
              <br>
              <a href="/papers/fsadeghi_VisKE.pdf"><metap>paper</metap></a> /
              <a href=""><metap>bibtex</metap></a>
              <p></p>
              <p>In this work, we introduce the problem of visual verification of relation phrases and developed a Visual Knowledge Extraction system called VisKE. Given a verb-based relation phrase between common nouns, our approach assess its validity by jointly analyzing over text and images and reasoning about the spatial consistency of the relative configurations of the entities and the relation involved. Our approach involves no explicit human supervision thereby enabling large-scale analysis. Using our approach, we have already verified over 12000 relation phrases. Our approach has been used to not only enrich existing textual knowledge bases by improving their recall, but also augment open domain question-answer reasoning.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src="./paper_figs/scenSun_teaser2.png" alt="scenSun" width="190" style="border-style: none"><br><br><br><br><br><br><br>
            <td width="70%" valign="middle">
              <a href="/papers/fsadeghi_CVPR14.pdf" id="MCG_journal">
              <papertitle>Incorporating Scene Context and Object Layout into Appearance Modeling</papertitle>
              </a>
              <br>
              <author>Hamid Izadinia<sup>*</sup>, </author><strong>Fereshteh Sadeghi<sup>*</sup></strong><author>, Ali Farhadi</author><br>
              <sup><b>*</b></sup>: equal contibution
              <br>
              <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, <span class="highlight">Spotlight</span> 2014              
              <br>
              <a href="/papers/fsadeghi_CVPR14.pdf"><metap>paper</metap></a> /
              <a href="/papers/fsadeghi_poster_CVPR14.pdf"><metap>poster</metap></a> /
              <a href="http://techtalks.tv/talks/incorporating-scene-context-and-object-layout-into-appearance-modeling/59789/"><metap>video</metap></a> /
              <a href=""><metap>bibtex</metap></a>
              <p></p>
              <p>In this paper, we propose a method to learn scene structures that can encode three main interlacing components of a scene: the scene category, the context-specific appearance of objects, and their layout. Our experimental evaluations show that our learned scene structures outperform state-of-the-art method of Deformable Part Models in detecting objects in a scene. Our scene structure provides a level of scene understanding that is amenable to deep visual inferences. The scene struc- tures can also generate features that can later be used for scene categorization. Using these features, we also show promising results on scene categorization.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src="./paper_figs/album_teaser2.png" alt="Album" width="190" style="border-style: none"><br><br><br><br><br><br><br><br>
            <td width="70%" valign="middle">
              <a href="/papers/fsadeghi_album_wacv15.pdf" id="MCG_journal">
              <papertitle>Learning to Select and Order Vacation Photograph</papertitle>
              </a>
              <br>
              <strong>Fereshteh Sadeghi</strong><author>, J. Rafael Tena, Ali Farhadi, Leonid Sigal</author>
              <br>
              <em>Winter Conference on Applications of Computer Vision (WACV)</em>, 2015              
              <br>
              <a href="/papers/fsadeghi_album_wacv15.pdf"><metap>paper</metap></a> /
              <a href="https://www.disneyresearch.com/publication/learning-to-select-and-order-vacation-photographs/"><metap>dataset</metap></a> /
              <a href=""><metap>bibtex</metap></a>
              <p></p>
              <p>we propose the problem of automatic photo album creation from an unordered image collection. To help solve this problem, we collect a new benchmark dataset based on Flicker images. We analyze the problem and provide experimental evidence, through user studies, that both selection and ordering of photos within an album is important for human observers. To capture and learn rules of album composition, we propose a discriminative structured model capable of encoding simple prefer ences for contextual layout of the scene and ordering between photos. The parameters of the model are learned using a structured SVM framework.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src="./paper_figs/fsadeghi_decoding_tex_encoding.png" alt="TextEncoding" width="190" style="border-style: none"><br><br><br><br><br><br><br><br>
            <td width="70%" valign="middle">
              <a href="/papers/fsadeghi_decoding_text_encoding.pdf" id="MCG_journal">
              <papertitle>Decoding the Text Encoding</papertitle>
              </a>
              <br>
              <strong>Fereshteh Sadeghi</strong><author>, Hamid Izadinia</author>
              <br>
              <em>arXiv:1412.6079</em>, 2014              
              <br>
              <a href="/papers/fsadeghi_decoding_text_encoding.pdf"><metap>paper</metap></a> /
              <a href=""><metap>bibtex</metap></a>
              <p></p>
              <p>Despite the attractiveness and simplicity of producing word clouds, they do not provide a thorough visualization for the distribution of the underlying data. Our proposed method is able to decode an input word cloud visualization and provides the raw data in the form of a list of (word, value) pairs. To the best of our knowledge our work is the first attempt to extract raw data from word cloud visualization. The results of our experiments show that our algorithm is able to extract the words and their weights effectively with considrerable low error rate.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src="./paper_figs/lpr_teaser.png" alt="LPR" width="190" style="border-style: none"><br><br><br><br><br><br><br><br>
            <td width="70%" valign="middle">
              <a href="/papers/fsadeghi_LPR_eccv12.pdf" id="MCG_journal">
              <papertitle>Latent pyramidal regions for recognizing scenes</papertitle>
              </a>
              <br>
              <strong>Fereshteh Sadeghi</strong><author>, Marshall Tappen</author>
              <br>
              <em>European Conference on Computer Vision (ECCV)</em>, 2012              
              <br>
              <a href="/papers/fsadeghi_LPR_eccv12.pdf"><metap>paper</metap></a> /
              <a href=""><metap>bibtex</metap></a>
              <p></p>
              <p>In this paper we proposed a simple but efficient image representation for solving the scene classification problem. Our new representation combines the benefits of spatial pyramid representation using nonlinear feature coding and latent Support Vector Machine (LSVM) to train a set of Latent Pyramidal Regions (LPR). Each of our LPRs captures a discriminative characteristic of the scenes and is trained by searching over all possible sub-windows of the images in a latent SVM training procedure. The final response of the LPRs form a single feature vector which we call the LPR representation and can be used for the classification task.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src="./paper_figs/labelTree_img.jpg" alt="LPR" width="190" style="border-style: none"><br><br><br><br>
            <td width="70%" valign="middle">
              <a href="/papers/fsadeghi-CVPR13.pdf" id="MCG_journal">
              <papertitle>Probabilistic Label Trees for Efficient Large Scale Image Classification</papertitle>
              </a>
              <br>
              <author>Baoyuan Liu, </author><strong>Fereshteh Sadeghi</strong><author>, Marshall Tappen, Ohad Shamir, Ce Liu</author>
              <br>
              <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2013              
              <br>
              <a href="/papers/fsadeghi-CVPR13.pdf"><metap>paper</metap></a> /
              <a href=""><metap>bibtex</metap></a>
              <p></p>
              <p>Large-scale recognition problems with thousands of classes pose a particular challenge because applying the classifier requires more computation as the number of classes grows. The label tree model integrates classification with the traversal of the tree so that complexity grows logarithmically. We show how the parameters of the label tree can be found using maximum likelihood estimation. This new probabilistic learning technique produces a label tree with significantly improved recognition accuracy.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src="./paper_figs/camsom.png" alt="camsom" width="190" style="border-style: none"><br><br><br><br>
            <td width="70%" valign="middle">
              <a href="" id="MCG_journal">
              <papertitle>A new active contour model based on the Conscience, Archiving
and Mean-Movement mechanisms and the SOM</papertitle>
              </a>
              <br>
              <strong>Fereshteh Sadeghi</strong><author>, H. Izadinia, R. Safabakhsh</author>
              <br>
              <em>Pattern Recognition Letters</em>, 2011  
              <em>IEEE Int. Conf. on Control, Automation, Robotics and Vision</em>, 2010 (earlier version)      
              <br>
              <a href=""><metap>paper</metap></a> /
              <a href=""><metap>bibtex</metap></a>
              <p></p>
              <p>In this paper we build upon self organizing map neural networks to develope an active contour model. We extend the Batch
                SOM method (BSOM) by introducing three mechanisms of Conscience, Archiving and Mean-Movement and test on number of grayscale complicated shapes including medical images. 
              </p><br><br><br><br>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src="./paper_figs/multi_epitopic.png" alt="camsom" width="190" style="border-style: none"><br><br><br><br>
            <td width="70%" valign="middle">
              <a href="" id="MCG_journal">
              <papertitle>A novel multi-epitopic immune network model hybridized with neural theory and fuzzy concept</papertitle>
              </a>
              <br>
              <author>H. Izadinia, </author><strong>Fereshteh Sadeghi</strong><author>,  M. M. Ebadzadeh</author>
              <br>
              <em>Neural Networks</em>, 2009 <br>             
              <em>International Joint Conference on Neural Networks</em>, 2009 (earlier version)
              <br>
              <a href=""><metap>paper</metap></a> /
              <a href=""><metap>bibtex</metap></a>
              <p></p>
              <p>Inspired from natural immune system and Jerne theory of immune network, in this paper we propose a multi-epitopic immune network model for pattern recognition. The proposed model is hybridized with Learning Vector Quantization (LVQ) and fuzzy set theory to present a new supervised learning method.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src="./paper_figs/fuzzyhough.png" alt="fuzzy" width="190" style="border-style: none"><br><br><br><br>
            <td width="70%" valign="middle">
              <a href="" id="MCG_journal">
              <papertitle>Fuzzy Generalized Hough Transform Invariant to Rotation and Scale in Noisy Environment</papertitle>
              </a>
              <br>
              <author>H. Izadinia, </author><strong>Fereshteh Sadeghi</strong><author>,  M. M. Ebadzadeh</author>
              <br>
              <em>International Conference on Fuzzy Systems</em>, 2009 
              <br>
              <a href=""><metap>paper</metap></a> /
              <a href=""><metap>bibtex</metap></a>
              <p></p>
              <p>In this paper we propose a Generalized Hough Transform technique that can handle rotation, scale and noise. Our method builds upon funzzy theory and impoves the classic Hough Transform technique via fuzzy inference. 
              </p><br>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src="./paper_figs/predictive_coding.png" alt="camsom" width="190" style="border-style: none"><br><br><br><br>
            <td width="70%" valign="middle">
              <a href="" id="MCG_journal">
              <papertitle>A New Secure Steganographic Method Based on Predictive Coding and Quantization Index Modulation</papertitle>
              </a>
              <br>
              <author>H. Izadinia, </author><strong>Fereshteh Sadeghi</strong><author>,  M. Rahmati</author>
              <br>
              <em>IEEE International Conference of Soft Computing and Pattern Recognition</em>, 2009 
              <br>
              <a href=""><metap>paper</metap></a> /
              <a href=""><metap>bibtex</metap></a>
              <p></p>
              <p>In this paper a new steganography method based on predictive coding which employs Quantization Index Modulation(QIM) is proposed. Furthermore, a correction mechanism is proposed to preserve the histogram of the cover image and make it resistant against histogram-based attacks. 
              </p><br>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src="./paper_figs/steganography.png" alt="camsom" width="190" style="border-style: none"><br><br><br><br>
            <td width="70%" valign="middle">
              <a href="" id="MCG_journal">
              <papertitle>A New Steganographic Method Using Quantization Index Modulation</papertitle>
              </a>
              <br>
              <author>H. Izadinia, </author><strong>Fereshteh Sadeghi</strong><author>,  M. Rahmati</author>
              <br>
              <em>IEEE International Conference on Computer and Automation Engineering</em>, 2009 
              <br>
              <a href=""><metap>paper</metap></a> /
              <a href=""><metap>bibtex</metap></a>
              <p></p>
              <p>This paper presents a new steganographic method based on predictive coding and embeds secret message in quantized error values via Quantization Index Modulation (QIM). The proposed method is superior to previous methods in that it can make a satisfying balance among the most concerned criteria in steganography which are imperceptibility, hiding capacity, compression ratio and robustness against attacks.
              </p><br>
            </td>
          </tr>

        </tbody></table>

        <hr class="featurette-divider">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Service and Professional Activities</heading><p></p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
 
          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <a href="http://www.roboticsfoundation.org/" ><img style="width:70%;max-width:100%" alt="Robotics" src="./paper_figs/robotics_logo.jpg" ></a>
            <td width="70%" valign="middle">
              <subheading>Robotics</subheading><br>
              <strong>Program committee/Reviewer member</strong><br>
              <em>&nbsp&nbsp Robotics: Science and Systems (RSS)</em>, 2019<br>
              <em>&nbsp&nbsp IEEE Conference on Robotics and Automation (ICRA) </em>, 2020, 2019, 2018, 2017<br>
              <em>&nbsp&nbsp IEEE Conference on Intelligent Robots and Systems (IROS)</em>, 2017<br>
              <em>&nbsp&nbsp Conference on Robotic Learning (CoRL)</em>, 2017, 2018, 2019<br>
              <em>&nbsp&nbsp IEEE Robotics and Automation Letters (RA-L)</em><br>
              <strong>Workshop co-organizer</strong><br>
              <em>&nbsp&nbsp Women in Robotics IV at Robotics: Science and Systems (RSS)</em>, 2018<br><br>
            </td>
          </tr>
         <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <a href="https://www.thecvf.com/" ><img style="width:70%;max-width:100%" alt="CVF" src="./paper_figs/cvf.jpg" ></a>
            <td width="70%" valign="middle">
              <subheading>Computer Vison</subheading><br>
              <strong>Program committee/Reviewer member</strong><br>
              <em>&nbsp&nbsp Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2020, 2019, 2018, 2017, 2016, 2015, 2014  <br>
              <em>&nbsp&nbsp International Conference on Computer Vision (ICCV)</em>, 2019, 2017 <br> 
              <em>&nbsp&nbsp European Conference on Computer Vision (ECCV)</em>, 2020, 2018, 2016, 2014    <br>    
              <em>&nbsp&nbsp Asian Conference on Computer Vision (ACCV)</em>, 2014, 2012     <br>    
              <em>&nbsp&nbsp Winter Conference on Applications of Computer Vision (WACV)</em>, 2016, 2015  <br> 
              <em>&nbsp&nbsp International Journal of Computer Vision (IJCV)</em>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src="./paper_figs/deep-learning-logo.png" alt="CVF" width="110" style="border-style: none">
            <td width="70%" valign="middle">
              <subheading>Machine Learning</subheading><br>
              <strong>Program committee/Reviewer member</strong><br>
              <em>&nbsp&nbsp International Conference on Learning Representations (ICLR)</em>, 2020, 2019 <br>           
              <em>&nbsp&nbsp Advances in Neural Information Processing Systems (NeurIPS)</em>, 2019, 2018  <br>   
              <em>&nbsp&nbsp IEEE Trans. on Neural Networks and Learning Systems</em>
            </td>
          </tr>
        </tbody></table>

        <hr class="featurette-divider">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Teaching</heading><p></p>              
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
         <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src="./paper_figs/robots.jpg" alt="" width="190" style="border-style: none">
              <a href="https://crmrkt.com/1NKWN2">source</a>
            <td width="70%" valign="middle">
              <strongBig><b>Artificial Intelligence</b></strongBig><br>
                <strong>Teaching Assistant and Guest Lecturer</strong><br>
                <em>&nbsp&nbsp <a href="http://courses.cs.washington.edu/courses/csep573/">CSEP 573 - Winter 2016</a></em><br>
                <em>&nbsp&nbsp <a href="https://courses.cs.washington.edu/courses/cse473/">CSE 473 - Autumn 2015</a> </em><br>
              <p>Search, Expectimax, CSP, MDP, Reinforcement Learning, Q-Learning, Uncertainty, Hidden Markov Models (HMMs), Baysian Networks (BNs), Naive Bayes, Perceptron, and fun PacMan game.</p>
              <strongBig><b>Machine Learning</b></strongBig><br>
              <strong>Teaching Assistant and Guest Lecturer</strong><br>
                <em>&nbsp&nbsp <a href="https://courses.cs.washington.edu/courses/csep546/16sp/">CSEP 546 - Spring 2016</a></em><br>
              <p>Learning theory, Bayesian learning, Neural networks, Support vector machines, Clustering and dimensionality reduction, Dimensionality reduction (PCA), Model ensembles </p>
              <strongBig><b>Computer Vision</b></strongBig><br>
              <strong>Instructor</strong><br>
                <em>&nbsp&nbsp <a href="https://courses.cs.washington.edu/courses/cse590v/15au/">Vision Seminar , CSE 590v - Autumn 2015</a></em><br>
                <em>&nbsp&nbsp <a href="https://courses.cs.washington.edu/courses/cse591g/15wi/">GRAIL Seminar , CSE 591 - Winter 2016</a></em><br>
            </td>
          </tr>

          </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <metap>~Thanks <a href="https://jonbarron.info/">Jon</a> for the nice tables!</a></metap>
              </p>
            </td>
          </tr>
        </tbody></table>



        <footer class="bs-docs-footer" role="contentinfo">
          <p style="color:DimGray"><center>&copy; Fereshteh Sadeghi</p>
        <p><a href="https://twitter.com/fereshteh_sa?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false">Follow @fereshteh_sa</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>
          <p><center>
            <a target="_blank" href="https://www.facebook.com/fereshteh.sadeghi"><img src="./paper_figs/fb.png" alt="Generic placeholder image" style="width: 25px; height: 25px;"></a>
            <a target="_blank" href="http://instagram.com/fereshs/"><img src="./paper_figs/insta.png" alt="Generic placeholder image" style="width: 22px; height: 22px;"></a>
        <a target="_blank" href="https://twitter.com/fereshteh_sa"><img src="./paper_figs/twitter.png" alt="Generic placeholder image" style="width: 30px; height: 30px;"></a> 
          </center></p>
        </footer>



      </td>
    </tr>
  </table>
</body>

</html>
