<html>
<head>
	<title>Fereshteh Sadeghi, UW CSE </title>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-10653444-5', 'auto');
  ga('send', 'pageview');

</script>

</head>
<link href='http://fonts.googleapis.com/css?family=Righteous' rel='stylesheet' type='text/css'>
<link rel="stylesheet" href="./codes/my2.css">
<link rel="stylesheet" href="./codes/mytheme2.css">
<script src="./codes/my.js"></script>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

<style>

.main-outer{
width: 70%;
border: 1px solid #ddd;
margin: 0 auto;
}
h1 {
background: #333;
color: #fff;
background-image: linear-gradient(57deg, transparent 20%, rgba(0,0,0,.2) 0%, rgba(0,0,0,.2) 79%, transparent 0%);
  z-index: -1;
padding: 20px;
text-align: center;
font-family: 'Righteous' !important;
font-style: normal;
font-weight: 400;
font-size: 64px;
margin-top: 0px;
}
h2{
font-family: 'Righteous' !important;
font-style: normal;
font-weight: 400;
}
.content {
font-family: 'Raleway';
font-style: normal;
font-weight: 400;
font-size: 18px;
color: #555;
border-bottom:1px solid #ddd;
padding-right:20px;
padding-left:20px;padding-bottom:20px;margin-bottom:20px;
}

img {
    width: 100%;
    height: auto;
}



pre {
    background-color:white;
    background-image:-webkit-linear-gradient(top, #f5f5f5 50%, white 50%);
    background-image:-moz-linear-gradient(top, #f5f5f5 50%, white 50%);
    background-image:-ms-linear-gradient(top, #f5f5f5 50%, white 50%);
    background-image:-o-linear-gradient(top, #f5f5f5 50%, white 50%);
    background-image:linear-gradient(top, #f5f5f5 50%, white 50%);
    -webkit-background-size:30px 30px;
    -moz-background-size:30px 30px;
    -ms-background-size:30px 30px;
    -o-background-size:30px 30px;
    background-size:30px 30px;
    background-repeat:repeat;
    font:bold 12px/15px "Inconsolata","Monaco","Consolas","Andale Mono","Bitstream Vera Sans Mono","Courier New",Courier,monospace;
    color:#333;
    border:2px solid #666;
    position:relative;
    padding:0 7px;
    margin:10px 0;
    overflow:auto;
    word-wrap:normal;
    white-space:pre;
    -webkit-box-shadow:0 1px 2px rgba(0,0,0,0.2);
    -moz-box-shadow:0 1px 2px rgba(0,0,0,0.2);
    box-shadow:0 1px 2px rgba(0,0,0,0.2);
    position:relative;
}

pre[data-codetype] {
    padding:29px 1em 7px 1em;
}

pre[data-codetype]:before {
    content:attr(data-codetype);
    display:block;
    position:absolute;
    top:0;
    right:0;
	width: 111%;
    left:0;
    background-color:#666;
    padding:0 7px;
    font:bold 11px/20px Arial,Sans-Serif;
    color:white;
}

pre[data-codetype="HTML"]       {border-color:#0B7E88;color:#08464B;}


pre[data-codetype="HTML"]:before          {background-color:#0B7E88;}
p{
font-family:arial;
}
</style>
<body>

<h1 class="content"></h1> 
<div class="container marketing">
<div class="row featurette">
        <div class="col-md-1"></div>
        <div class="col-md-4">
        <img class="featurette-image img-responsive" src="./pics/fsadeghi.jpg" alt="Generic placeholder image" style="width: 320px; height: 373px;">
        </div>
        <div class="col-md-6">
          <h4 class="featurette-heading"><span class="text-muted" style="color:#000000"><font size="6" > <b>Fereshteh Sadeghi <b></span></h4>
          <p><a href="https://twitter.com/fereshteh_sa?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false">Follow @fereshteh_sa</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>
          <h4 class="featurette-heading"><span class="text-muted">Computer Science Ph.D , University of Washington</span></h4>
          <!-- <h4 class="featurette-heading"><span class="text-muted">University of Washington</span></h4> -->
          <h5 class="featurette-heading"><span class="text-muted">     fsadeghi ~at~ cs.washington.edu          </span></h5>
	  
	<h5 class="featurette-heading">  <span style="color:DarkOrchid"><font size="3" >  <a href="https://digital.lib.washington.edu/researchworks/bitstream/handle/1773/43660/Sadeghi_washington_0250E_19800.pdf?sequence=1&isAllowed=y">PhD Thesis</a>    </span><a target="_blank" href="https://digital.lib.washington.edu/researchworks/bitstream/handle/1773/43660/Sadeghi_washington_0250E_19800.pdf?sequence=1&isAllowed=y"><img class="img-circle" src="./paper_figs/logo-uw-2011.png" alt="Generic placeholder image" style="width: 25px; height: 25px;"></a>  &nbsp/&nbsp  <span style="color:#009933"><font size="3" >   <a href="http://scholar.google.com/citations?user=vS8b6GwAAAAJ&hl=en">  Google Scholar  </a>        </span><a target="_blank" href="http://scholar.google.com/citations?user=vS8b6GwAAAAJ&hl=en"><img class="img-circle" src="./paper_figs/ghat.png" alt="Generic placeholder image" style="width: 25px; height: 25px;"></a>  </h5>

          
          <p  class="lead">My research is focused on developing learning algorithms that combine perception and control for learning robot skills. I am interested in how learning can be used to enable machines acquire behavioral skills that can generalize to unstructured real world settings. During my PhD, I have developed techniques for learning highly generalizable vision-based robot controllers in simulation for efficient transfer and adaptability to the real world. </p>
        </div>

</div>
<p></p>
<div class="page-header">
<p><span style="color:#006600"><font size="6" > News! </span></p>
<p> <span style="color:#009933"><font size="3" ><a href="https://arxiv.org/pdf/1908.08031.pdf" ><img class="img-circle" src="./paper_figs/mushr_small.png" alt="Generic placeholder image" style="width: 45px; height: 45px;"></a> ~ Aug. 2019 ~ </span>  <a href="https://arxiv.org/pdf/1908.08031.pd"></a> Excited to release <a href="https://news.cs.washington.edu/2019/08/21/allen-school-releases-mushr-robotic-race-car-platform-to-drive-advances-in-ai-research-and-education/">MuSHR! </a> Also featured in <a href="https://www.geekwire.com/2019/robotic-race-car-platform-univ-washington-designed-speed-research-around/">GeekWire</a> and <a href="https://www.wired.com/story/small-cars-help-drive-autonomous-future/?mbid=social_tw_transport&utm_brand=wired&utm_campaign=wiredtransportation&utm_medium=social&utm_social-type=owned&utm_source=twitter"> WIRED</a>!</p>
<p> <span style="color:#009933"><font size="3" ><a href="https://youtu.be/UAKQhciYN6w " ><img class="img-circle" src="./paper_figs/rss2019.png" alt="Generic placeholder image" style="width: 45px; height: 45px;"></a> ~ May. 2019 ~ </span>  <a href="https://arxiv.org/pdf/1902.05947.pdf"></a> <a href="https://youtu.be/UAKQhciYN6w ">DIViS: Domain Invariant Visual Servoing for Collision-Free Goal Reaching</a> is accepted in <a href="http://www.roboticsconference.org/">RSS 2019 </a> conference! </p>
<p> <span style="color:#009933"><font size="3" ><a href="https://digital.lib.washington.edu/researchworks/bitstream/handle/1773/43660/Sadeghi_washington_0250E_19800.pdf?sequence=1&isAllowed=y" ><img class="img-circle" src="./paper_figs/huskey_grad.png" alt="Generic placeholder image" style="width: 45px; height: 45px;"></a> ~ Feb. 2019 ~ </span> Successfully defended my PhD thesis: <a href="https://digital.lib.washington.edu/researchworks/handle/1773/43660">"Domain Invariant and Semantic Aware Visual Servoing" </a> and am officially a Doctor in<a href="https://www.cs.washington.edu/">  CS  </a>! </p>
<p> <span style="color:#009933"><font size="3" ><a href="https://youtu.be/UAKQhciYN6w " ><img class="img-circle" src="./paper_figs/thumbnail_DIViS.png" alt="Generic placeholder image" style="width: 45px; height: 45px;"></a> ~ Feb. 2019 ~ </span> New <a href="https://arxiv.org/pdf/1902.05947.pdf">arXiv</a> preprint <a href="https://youtu.be/UAKQhciYN6w ">DIViS: Domain Invariant Visual Servoing for Collision-Free Goal Reaching</a> posted! </p>
<p> <span style="color:#009933"><font size="3" ><a href="https://www.ri.cmu.edu/event/acquiring-and-transferring-generalizable-vision-based-robot-skills/" ><img class="img-circle" src="./paper_figs/RI_VASC.jpg" alt="Generic placeholder image" style="width: 45px; height: 45px;"></a> ~ Oct. 2018 ~ </span> I will be giving an invited talk at the <a href="https://www.ri.cmu.edu/event/acquiring-and-transferring-generalizable-vision-based-robot-skills/">RI VASC Seminar.</a></p>
<p> <span style="color:#009933"><font size="3" ><a href="https://ai.googleblog.com/2018/06/teaching-uncalibrated-robots-to_22.html" ><img class="img-circle" src="./paper_figs/GoogleAI.png" alt="Generic placeholder image" style="width: 45px; height: 45px;"></a> ~ Jun. 2018 ~ </span> I wrote a <a href="https://ai.googleblog.com/2018/06/teaching-uncalibrated-robots-to_22.html">GoogleAI </a> blog post :  <a href="https://ai.googleblog.com/2018/06/teaching-uncalibrated-robots-to_22.html">Teaching Uncalibrated Robots to Visually Self-Adapt</a>.</p>

<p> <span style="color:#009933"><font size="3" ><a href="https://sites.google.com/view/rss2018women/" ><img class="img-circle" src="./paper_figs/RSS18.png" alt="Generic placeholder image" style="width: 45px; height: 45px;"></a> ~ Jun. 2018 ~ </span> I am co-organizing the <a href="https://sites.google.com/view/rss2018women/">4th Women in Robotics Workshop </a> at <a href="http://www.roboticsconference.org/">Robotics: Science and Systems Conference(R:SS)</a>.</p>

<p> <span style="color:#009933"><font size="3" ><a href="https://fsadeghi.github.io/Sim2RealViewInvariantServo/" ><img class="img-circle" src="./paper_figs/cvpr18logo_3.jpg" alt="Generic placeholder image" style="width: 45px; height: 45px;"></a> ~ Feb. 2018 ~ </span> <a href="https://www.youtube.com/watch?v=oLgM2Bnb7fo&feature=youtu.be">Sim2Real View Invariant Visual Servoing </a> accepted to <a href="http://cvpr2018.thecvf.com/">CVPR2018</a>.</p>

<p> <span style="color:#009933"><font size="3" ><a href="https://fsadeghi.github.io/Sim2RealViewInvariantServo/" ><img class="img-circle" src="./paper_figs/sim2realviservo.jpg" alt="Generic placeholder image" style="width: 45px; height: 45px;"></a> ~ Dec. 2017 ~ </span> Sim2Real View Invariant Visual Servoing posted on <a href="https://arxiv.org/pdf/1712.07642.pdf">arxiv </a>. See the video <a href="https://www.youtube.com/watch?v=oLgM2Bnb7fo&feature=youtu.be">here </a>.</p>

<p> <span style="color:#009933"><font size="3" ><a href="https://www.iros2017.org/" ><img class="img-circle" src="./paper_figs/iros.jpg" alt="Generic placeholder image" style="width: 45px; height: 45px;"></a> ~ Sept. 2017 ~ </span> At IROS 2017, I will be giving an invited talk in the workshop on <a href="http://www.seas.upenn.edu/~loiannog/workshopIROS2017uav/index.html#program">Vision-based Agile Autonomous Navigation of UAVs </a>.</p>

<p> <span style="color:#009933"><font size="3" ><a href=https://research.google.com/teams/brain/" ><img class="img-circle" src="./paper_figs/brain.jpg" alt="Generic placeholder image" style="width: 45px; height: 45px;"></a> ~ June. 2017 ~ </span>I am spending time at  <a href="https://research.google.com/teams/brain/robotics/">Google Brain Robotics </a> for internship</p>
<p> <span style="color:#009933"><font size="3" ><img class="img-circle" src="./paper_figs/nvidia.jpg" alt="Generic placeholder image" style="width: 45px; height: 45px;"> ~ May. 2017 ~ </span> Won the <a href="http://research.nvidia.com/grad_fellows/2017/FereshtehSadeghi">NVIDIA Graduate Fellowship 2017- 2018</a> award! </p>
<p> <span style="color:#009933"><font size="3" ><a href=http://www.roboticsconference.org/program/papers/13/" ><img class="img-circle" src="./paper_figs/RSS.png" alt="Generic placeholder image" style="width: 45px; height: 45px;"></a> ~ April. 2017 ~ </span> Our <a href="https://arxiv.org/pdf/1611.04201.pdf">CAD<sup>2</sup>RL paper</a> got accepted in <a href="http://www.roboticsconference.org/">Robotics: Science and Systems Conference(R:SS) !! </a> Check the <a href="https://fsadeghi.github.io/CAD2RL/">video</a> here. </p>
<p> <span style="color:#009933"><font size="1" > ~ Nov. 2016 ~ </span><a href="https://arxiv.org/pdf/1611.04201.pdf">New preprint</a> on deep reinforcement learning and deep robotic learning posted! Check the <a href="https://fsadeghi.github.io/CAD2RL/">video</a> here. </p>
<p> <span style="color:#009933"><font size="1" > ~ Nov. 2016 ~ </span> At <a href="http://nips.cc/">NIPS'16</a>, I will be giving a contributed talk in the <a href="https://sites.google.com/site/nips16interaction/">DLAI</a> workshop </p>
<p> <span style="color:#009933"><font size="1" > ~ Oct. 2016 ~ </span> I will be serving as a program committee member in <a href="http://cvpr2017.thecvf.com/">CVPR'17</a>.</p>
<p> <span style="color:#009933"><font size="1" > ~ Jun. 2016 ~ </span> Spending my summer as a research intern at <a href="https://www.magicleap.com/">Magic Leap</a> ! It feels magical to be a leaper :) <a href="https://www.magicleap.com/#/company" ><img class="img-circle" src="./paper_figs/leaper.png" alt="Generic placeholder image" style="width: 30px; height: 30px;"></a> </p>
<p> <span style="color:#009933" ><font size="1" > ~ Apr. 2016 ~ </span> I am guest lecturer at ML course (CSEP546) for Spring'16 quarter.</font></p>
<p> <span style="color:#009933"><font size="1" > ~ Feb. 2016 ~ </span> I will be serving as a program committee member in <a href="http://www.eccv2016.org/">ECCV'16</a>.</font></p>
<p> <span style="color:#009933"><font size="1" > ~ Feb. 2016 ~ </span> I am guest lecturer at AI course (CSEP573) for Winter'16 quarter.</font></p>
<p> <span style="color:#009933"><font size="1" > ~ Nov. 2015 ~ </span> I will be serving as a program committee member in <a href="http://www.pamitc.org/cvpr16/">CVPR'16</a>.</font></p>
<p> <span style="color:#009933"><font size="1" > ~ Oct. 2015 ~ </span> I am guest lecturer at AI course (CSE473) for Fall'15 quarter.</font></p>
<p> <span style="color:#009933"><font size="1" > ~ Sep. 2015 ~ </span>I am organizing <a href="https://courses.cs.washington.edu/courses/cse590v/15au/">Vision Seminar (CSE 590v)</a> for the Fall'15 quarter.</font></p>
<p> <span style="color:#009933"><font size="1" > ~ Sep. 2015 ~ </span> Our Visual analogy paper got accepted in <a href="https://nips.cc/Conferences/2015">NIPS'15</a>.</font></p>
<p> <span style="color:#009933"><font size="1" > ~ Sep. 2015 ~ </span> Our Visual entailment paper got accepted as oral in <a href="http://pamitc.org/iccv15/">ICCV'15</a>.</font></p>
<p> <span style="color:#009933"><font size="1" > ~ Mar. 2015 ~ </span> VisKE got accepted in <a href="http://www.pamitc.org/cvpr15/">CVPR'15</a>.</font></p>
<p> <span style="color:#009933"><font size="1" > ~ Jan. 2015 ~ </span> My album creation paper is featured in <a href="http://www.eurekalert.org/pub_releases/2015-01/dr-drc010715.php">EurekAlert</a>.</p></li>
<p> <span style="color:#009933"><font size="1" > ~ Dec. 2014 ~ </span> I will organize the GRAIL seminar (cse 591) for the Spring'15 quarter. </font></p>
<p> <span style="color:#009933"><font size="1" > ~ Nov. 2014 ~ </span> I will be serving as a program committee member in <a href="http://www.pamitc.org/cvpr15/">CVPR'15</a>.</font></p>
<hr class="featurette-divider">
<div class="page-header">
<h2><span style="color:#006600">Talks! </span></h2>

 <div class="container marketing">
      <div class="row featurette">
        <div class="col-md-3">
          <div class="col-md-3" id="about-media">	  
	<center>
	   <iframe width="373" height="210" src="https://www.youtube.com/embed/rovQlRFSawo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  </center>
	   </div>

	</div>

          <div class="col-md-9">
          <div class="col-md-9" id="about-media">   
  <center>
     <iframe width="373" height="210" src="https://www.youtube.com/embed/jT1p1HAiDCg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  </center>
     </div>
  </div>

  </div>

  </div>


<div>

<div class="row featurette">
<div class="col-md-3">
<p><font size="2" >&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://www.roboticsproceedings.org/rss15/p55.pdf">Robotics: Science and Systems 2019</a> </font></p>
</div>

<div class="col-md-9">
<p><font size="2" >&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://arxiv.org/pdf/1712.07642.pdf">UC Berkeley BAIR/BDD Seminar Series</a> </font></p>
</div>

</div>
</div>
<p> <span style="color:#009933"><font size="4" ><img class="img-circle" src="./paper_figs/star.png" alt="Generic placeholder image" style="width: 45px; height: 30px;"> ~ June. 2019 ~ </span>  <a href="https://www.youtube.com/watch?v=rovQlRFSawo">Robotics: Science and Systems Conference 2019</a>  </p>
  <p> <span style="color:#009933"><font size="4" ><img class="img-circle" src="./paper_figs/star.png" alt="Generic placeholder image" style="width: 45px; height: 30px;"> ~ June. 2019 ~ </span>  <a href="https://sim2real.github.io/">Workshop on Closing the Reality Gap in Sim2real at RSS</a>  </p>
<p> <span style="color:#009933"><font size="2" > ~ October. 2018 ~ </span>  <a href=https://www.ri.cmu.edu/event/acquiring-and-transferring-generalizable-vision-based-robot-skills/">RI VASC Seminar</a> </p>
<p> <span style="color:#009933"><font size="2" > ~ April. 2018 ~ </span>  <a href="http://citris-uc.org/bair-seminar-series/">BAIR Seminar</a>, See the video <a href="https://www.youtube.com/watch?v=jT1p1HAiDCg">here </a></p> </p>
<p> <span style="color:#009933"><font size="2" > ~ March. 2018 ~ </span>  <a href="http://www.gputechconf.com/">GPU Technology Conference 2018</a>  </p>
  <p> <span style="color:#009933"><font size="2" > ~ July. 2017 ~ </span>  <a href="http://www.roboticsproceedings.org/rss13/p34.pdf">Robotics: Science and Systems Conference 2017</a>  </p>
<p> <span style="color:#009933"><font size="2" > ~ May. 2017 ~ </span>  <a href="http://www.gputechconf.com/">GPU Technology Conference 2017</a>, See the video<a href="https://youtu.be/qFmH4oZPlYY"> here</a> </p>
<p> <span style="color:#009933"><font size="2" > ~ May. 2017 ~ </span>  <a href="http://sorl.citris-uc.org/">Symposium on Robot Learning 2017</a>  </p>
<p> <span style="color:#009933"><font size="2" > ~ April. 2017 ~ </span>  <a href="http://citris-uc.org/bair-seminar-series/">BAIR Seminar</a>  </p>
<p> <span style="color:#009933"><font size="2" > ~ Dec. 2016 ~ </span>  <a href="https://sites.google.com/site/nips16interaction/">Deep Learning for Action and Interaction, NIPS 2016</a>, Talk video <a href="https://youtu.be/HpmA4t0NWks">here</a>  </p>


<div class="page-header">
<h2><span style="color:#006600">Videos! </span></h2>

 <div class="container marketing">
      <div class="row featurette">
        <div class="col-md-3">
          <div class="col-md-3" id="about-media">   
  <center>
     <iframe width="373" height="210" src="https://www.youtube.com/embed/UAKQhciYN6w" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  </center>
     </div>

  </div>

          <div class="col-md-9">
          <div class="col-md-9" id="about-media">   
  <center>
     <iframe width="373" height="210" src="https://www.youtube.com/embed/oLgM2Bnb7fo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  </center>
     </div>
  </div>

  </div>

  <div class="row featurette">
<div class="col-md-3">
<p><font size="2" >&nbsp;&nbsp;<a href="http://www.roboticsproceedings.org/rss15/p55.pdf">DIViS: Domain Invariant Visual Servoing</a> </font></p>
</div>

<div class="col-md-9">
<p><font size="2" >&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Sadeghi_Sim2Real_Viewpoint_Invariant_CVPR_2018_paper.pdf">Sim2Real Viewpoint Invariant Visual Servoing by Recurrent Control</a></font></p>
</div>

</div>

  </div>

<div class="container marketing">
      <div class="row featurette">
        <div class="col-md-3">
          <div class="col-md-3" id="about-media">   
  <center>
     <iframe width="373" height="210" src="https://www.youtube.com/embed/nXBWmzFrj5s" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  </center>
     </div>

  </div>
  </div>

<div class="row featurette">
<div class="col-md-3">
<p><font size="2" >&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://www.roboticsproceedings.org/rss13/p34.pdf">CAD2RL, Simulation Randomization &nbsp;&nbsp;&nbsp;&nbsp;(a.k.a. Domain Randomization)</a> </font></p>
</div>
</div>

</div>


<h2>PhD Thesis </h2>
<hr class="featurette-divider">
<div class="row featurette">
        <div class="col-md-1">
          <p></p>
          <a href="https://digital.lib.washington.edu/researchworks/bitstream/handle/1773/43660/Sadeghi_washington_0250E_19800.pdf?sequence=1&isAllowed=y
" target="_blank" ><img class="featurette-image img-responsive" src="./paper_figs/logo-uw-2011.png" alt="Generic placeholder image" style="width: 80px; height: 80px;"></a>
        </div>
        <div class="col-md-6">
          <h4><b>Domain Invariant and Semantic Aware Visual Servoing</b></h4>
          <p> <span style="color:DimGray"><font size="3" > </span></p>
<p> <span style="color:DarkOrchid"><font size="4" > <b>Fereshteh Sadeghi</b> </span><span style="color:#2874A6"><font size="3" >  </span></p></p>  
          <h5 style="color:DarkCyan">  <a href="https://digital.lib.washington.edu/researchworks/handle/1773/43660"> Ph.D. thesis, University of Washington, 2019</a> </h5>
        </div>
      </div>

<!--<hr class="featurette-divider"> -->

<h2>Preprints </h2>
<hr class="featurette-divider">
  <div class="row featurette">
        <div class="col-md-3">
          <p></p>
          <a href="https://arxiv.org/pdf/1908.08031.pdf" target="_blank" ><img class="featurette-image img-responsive" src="./paper_figs/mushr.jpg" alt="Generic placeholder image" style="width: 290px; height: 210px;"></a>
        </div>
        <div class="col-md-9">
          <h4>MuSHR: A Low-Cost, Open-Source Robotic Racecar for Education and Research</h4>
          <p style="color:DimGray">MuSHR is a low-cost, open-source robotic racecar platform for education and research. MuSHR aspires to contribute towards democratizing the field of robotics as a low-cost platform that can be built and deployed by following detailed, open documentation and do-it-yourself tutorials.</p>
<p> <span style="color:#2874A6"><font size="3" > S. Srinivasa, P. Lancaster, J. Michalove, M. Schmittle, C. Summers, M. Rockett, J. R. Smith, S. Choudhury, C. Mavrogiannis, </span><span style="color:DarkOrchid"><font size="4" > <b>Fereshteh Sadeghi</b> </span></p>  
          <!--<h5 style="color:DarkOrchid "><b>Fereshteh Sadeghi</b>, C. Lawrence Zitnick, Ali Farhadi</h5>-->
          <h5 style="color:DarkCyan">arXiv preprint arXiv:1908.08031 (2019)</h5>
          <p><a target="_blank" href="https://arxiv.org/pdf/1908.08031.pdf" class="btn btn-xs btn-success btn" role="button">arXiv</a>
          <a target="_blank" href="https://github.com/prl-mushr" class="btn btn-xs btn-info btn" role="button">Github</a>
<a class="btn btn-xs btn-primary" href="https://mushr.io/" role="button">Webpage</a>
        </div>
      </div>


<h2>Publications </h2>
<hr class="featurette-divider">
<p  class="lead">Here is the list of my most recent publications. For the complete list of my publications please visit my <a target="_blank" href="http://scholar.google.com/citations?user=vS8b6GwAAAAJ&hl=en">Google Scholar </a> page and <a target="_blank" href="http://www.informatik.uni-trier.de/~ley/pers/hd/s/Sadeghi:Fereshteh.html">DBLP</a>.</p>
<h3>    </h3>
<h3>    </h3>
<h3>    </h3>
<p>      </p>
<p>      </p>

<div class="row featurette">
        <div class="col-md-3">
          <p></p>
          <a href="https://youtu.be/UAKQhciYN6w" target="_blank" ><img class="featurette-image img-responsive" src="./paper_figs/teaser_DIViS.png" alt="Generic placeholder image" style="width: 290px; height: 210px;"></a>
        </div>
        <div class="col-md-9">
          <h4><b>DIViS: Domain Invariant Visual Servoing for Collision-Free Goal Reaching</b></h4>
          <p> <span style="color:DimGray"><font size="3" > In this paper, we investigate how to minimize human effort and intervention to teach robots perform real world tasks that incorporate semantics and we propose DIViS, a Domain Invariant policy learning approach for collision free Visual Servoing. While DIViS does not use any real robot data at the training time it is capable of servoing real mobile robots to semantic object categories in many diverse and unstructured real-world environments.</span></p>
<p> <span style="color:DarkOrchid"><font size="4" > <b>Fereshteh Sadeghi</b> </span><span style="color:#2874A6"><font size="3" >  </span></p></p>  
          <h5 style="color:DarkCyan">  <a href="http://www.roboticsconference.org/">RSS (Robotics: Science and System) 2019</a> </h5>
          <p><a target="_blank" href="https://arxiv.org/pdf/1902.05947.pdf" class="btn btn-xs btn-success btn" role="button">arXiv</a>
          <a target="_blank" href="https://youtu.be/UAKQhciYN6w" class="btn btn-xs btn-info btn" role="button">Video</a>
<a class="btn btn-xs btn-primary" href="https://fsadeghi.github.io/DIViS/" role="button">Project page</a></p>
        </div>
      </div>


<div class="row featurette">
        <div class="col-md-3">
          <p></p>
          <a href="https://www.youtube.com/watch?v=oLgM2Bnb7fo" target="_blank" ><img class="featurette-image img-responsive" src="./paper_figs/sim2realviservo.png" alt="Generic placeholder image" style="width: 290px; height: 210px;"></a>
        </div>
        <div class="col-md-9">
          <h4><b>Sim2Real View Invariant Visual Servoing by Recurrent Control</b></h4>
          <p style="color:DimGray">In this paper, we study how viewpoint-independent visual servoing skills can be learned automatically in a robotic manipulation scenario. To this end, we train a deep, recurrent controller that can automatically determine which actions move the end-point of a robotic arm to a desired object. We show how we can learn this recurrent controller using simulated data, and then describe how the resulting model can be transferred to a real-world Kuka IIWA robotic arm. </p>
<p> <span style="color:DarkOrchid"><font size="4" > <b>Fereshteh Sadeghi</b> </span><span style="color:#2874A6"><font size="3" > , Alexander Toshev, Eric Jang, Sergey Levine </span></p>  
          <!-- <h5 style="color:DarkOrchid "><b>Fereshteh Sadeghi</b>, Alexander Toshev, Eric Jang, Sergey Levine</h5> -->
          <h5 style="color:DarkCyan">  <a href="http://cvpr2018.thecvf.com/">CVPR 2018</a> </h5>
          <p><a target="_blank" href="https://arxiv.org/pdf/1712.07642.pdf" class="btn btn-xs btn-success btn" role="button">arXiv</a>
          <a target="_blank" href="https://www.youtube.com/watch?v=oLgM2Bnb7fo&feature=youtu.be" class="btn btn-xs btn-info btn" role="button">Video</a>
<a class="btn btn-xs btn-primary" href="https://www.youtube.com/watch?v=jT1p1HAiDCg" role="button">Talk</a></p>
        </div>
      </div>
      
<div class="row featurette">
        <div class="col-md-3">
          <p></p><a target="_blank" href="https://www.youtube.com/watch?v=nXBWmzFrj5s" >
          <img class="featurette-image img-responsive" src="./paper_figs/CAD2RL.png" alt="Generic placeholder image" style="width: 290px; height: 210px;"></a>
        </div>
        <div class="col-md-9">
          <h4>CAD<sup>2</sup>RL : Real Single-Image Flight without a Single Real Image</h4>
          <p style="color:DimGray">We propose CAD<sup>2</sup>RL, a flight controller for Collision Avoidance via Deep Reinforcement Learning that can be used to perform collision-free flight in the real world although it is trained entirely in a 3D CAD model simulator. Our method uses only single RGB images from a monocular camera mounted on the robot as the input and is specialized for indoor hallway following and obstacle avoidance.</p>
<p> <span style="color:DarkOrchid"><font size="4" > <b>Fereshteh Sadeghi</b> </span><span style="color:#2874A6"><font size="3" > and Sergey Levine </span></p>          
<!--<h5 style="color:DarkOrchid "><b>Fereshteh Sadeghi</b> and  Sergey Levine</h5>-->
          <h5 style="color:DarkCyan">  <a href="http://www.roboticsconference.org/">RSS (Robotics: Science and System) 2017</a> </h5>
          <p><a target="_blank" href="https://arxiv.org/pdf/1611.04201.pdf" class="btn btn-xs btn-success btn" role="button">arXiv</a>
          <a target="_blank" href="https://fsadeghi.github.io/CAD2RL/" class="btn btn-xs btn-info btn" role="button">project page</a></p>
        </div>
      </div>


      <div class="row featurette">
        <div class="col-md-3">
          <p></p>
          <img class="featurette-image img-responsive" src="./paper_figs/Visalogy_teaser.png" alt="Generic placeholder image" style="width: 290px; height: 210px;">
        </div>
        <div class="col-md-9">
          <h4>VISALOGY: Answering Visual Analogy Questions</h4>
          <p style="color:DimGray">In this paper, we study the problem of answering visual analogy questions. These questions take the form of image A is to image B as image C is to what. Answering these questions entails discovering the mapping from image A to image B and then extending the mapping to image C and searching for the image D such that the relation from A to B holds for C to D. We pose this problem as learning an embedding that encourages pairs of analogous images with similar transformations to be close together using convolutional neural networks with a quadruple Siamese architecture.</p>
<p> <span style="color:DarkOrchid"><font size="4" > <b>Fereshteh Sadeghi</b> </span><span style="color:#2874A6"><font size="3" > , C. Lawrence Zitnick, Ali Farhadi </span></p>  
          <!--<h5 style="color:DarkOrchid "><b>Fereshteh Sadeghi</b>, C. Lawrence Zitnick, Ali Farhadi</h5>-->
          <h5 style="color:DarkCyan">NIPS 2015</h5>
          <p><a target="_blank" href="http://arxiv.org/pdf/1510.08973.pdf" class="btn btn-xs btn-success btn" role="button">arXiv</a>
          <a target="_blank" href="./papers/fsadeghi_nips_poster_visual_analogy.pdf" class="btn btn-xs btn-primary btn" role="button">poster</a>
          <a target="_blank" href="./papers/fsadeghi_visual_analogy_slides.pdf" class="btn btn-xs btn-info btn" role="button">slides</a></p>
        </div>
      </div>


      <div class="row featurette">
        <div class="col-md-3">
          <p></p>
          <img class="featurette-image img-responsive" src="./paper_figs/VisualEntailment_teaser.jpg" alt="Generic placeholder image" style="width: 290px; height: 170px;">
        </div>
        <div class="col-md-9">
          <h4>Segment-Phrase Table for Semantic Segmentation, Visual Entailment and Paraphrasing</h4>
          <p style="color:DimGray">We introduce Segment-Phrase Table (SPT), a large collection of bijective associations between textual phrases and their corresponding segmentations. We show that fine-grained textual labels facilitate contextual reasoning that helps in satisfying semantic constraints across image segments. This feature enables us to achieve state-of-the-art segmentation results on benchmark datasets. We also show that the association of high-quality segmentations to textual phrases aids in richer semantic understanding and reasoning of these textual phrases which motivates the problem of visual entailment and visual paraphrasing.</p>
<p> <span style="color:#2874A6"><font size="3" > Hamid Izadinia, </span><span style="color:DarkOrchid"><font size="4" > <b>Fereshteh Sadeghi</b> </span><span style="color:#2874A6"><font size="3" > , Santosh K. Divvala, Yejin Choi, Ali Farhadi </span></p>
          <h5 style="color:DarkCyan">ICCV 2015 (Oral)</h5>
          <p><a target="_blank" href="http://arxiv.org/pdf/1509.08075v1.pdf" class="btn btn-xs btn-success btn" role="button">paper</a>
             <!--<a target="_blank" href="./papers/fsadeghi_VisKE_ext.pdf" class="btn btn-xs btn-primary btn" role="button">One-page summary</a> -->
             </p>
        </div>
      </div>


      <div class="row featurette">
        <div class="col-md-3">
          <p></p>
          <img class="featurette-image img-responsive" src="./paper_figs/VisKE_teasor.png" alt="Generic placeholder image" style="width: 290px; height: 210px;">
        </div>
        <div class="col-md-9">
          <h4>VisKE: Visual Knowledge Extraction and Question Answering by Visual Verification of Relation Phrases </h4>
          <p style="color:DimGray">In this work, we introduce the problem of visual verification of relation phrases and developed a Visual Knowledge Extraction system called VisKE. Given a verb-based relation phrase between common nouns, our approach assess its validity by jointly analyzing over text and images and reasoning about the spatial consistency of the relative configurations of the entities and the relation involved. Our approach involves no explicit human supervision thereby enabling large-scale analysis. Using our approach, we have already verified over 12000 relation phrases. Our approach has been used to not only enrich existing textual knowledge bases by improving their recall, but also augment open domain question-answer reasoning.</p>
<p> <span style="color:DarkOrchid"><font size="4" > <b>Fereshteh Sadeghi</b> </span><span style="color:#2874A6"><font size="3" > , Santosh K. Divvala, Ali Farhadi </span></p>            
<!-- <h5 style="color:DarkOrchid "><b>Fereshteh Sadeghi</b>, Santosh K. Divvala, Ali Farhadi</h5> -->
          <h5 style="color:DarkCyan">CVPR 2015</h5>
          <p><a target="_blank" href="./papers/fsadeghi_VisKE.pdf" class="btn btn-xs btn-success btn" role="button">paper</a>
             <!--<a target="_blank" href="./papers/fsadeghi_VisKE_ext.pdf" class="btn btn-xs btn-primary btn" role="button">One-page summary</a> -->
             <a class="btn btn-xs btn-primary" href="https://github.com/fsadeghi/VisKE" role="button">Data</a></p>
        </div>
      </div>

      <div class="row featurette">
        <div class="col-md-3">
          <p></p>
          <img class="featurette-image img-responsive" src="./paper_figs/scenSun_teaser2.png" alt="Generic placeholder image" style="width: 250px; height: 220px;">
        </div>
        <div class="col-md-9">
          <h4>Incorporating Scene Context and Object Layout into Appearance Modeling </h4>
          <p style="color:DimGray">In this paper, we propose a method to learn scene structures that can encode three main interlacing components of a scene: the scene category, the context-specific appearance of objects, and their layout. Our experimental evaluations show that our learned scene structures outperform state-of-the-art method of Deformable Part Models in detecting objects in a scene. Our scene structure provides a level of scene understanding that is amenable to deep visual inferences. The scene struc- tures can also generate features that can later be used for scene categorization. Using these features, we also show promising results on scene categorization.</p>
<p> <span style="color:#2874A6"><font size="3" > Hamid Izadinia<sup>*</sup> </span><span style="color:DarkOrchid"><font size="4" > <b>, Fereshteh Sadeghi<sup>*</sup></b> </span><span style="color:#2874A6"><font size="3" > , Ali Farhadi </span></p>
          <h8 style="color:MediumOrchid"><sup><b>*</b></sup>: equal contibution</h8>
          <h5 style="color:DarkCyan">CVPR 2014</h5>
          <p><a target="_blank" href="./papers/fsadeghi_CVPR14.pdf" class="btn btn-xs btn-success btn" role="button">paper</a>
             <a target="_blank" href="./papers/fsadeghi_poster_CVPR14.pdf" class="btn btn-xs btn-primary btn" role="button">poster</a>
             <a target="_blank" href="http://techtalks.tv/talks/incorporating-scene-context-and-object-layout-into-appearance-modeling/59789/" class="btn btn-xs btn-info btn" role="button">Spotlight</a></p>
        </div>
      </div>

      <hr class="featurette-divider">
      <div class="row featurette">
        <div class="col-sm-3">
          <p>     </p>
          <p>     </p>
          <img class="featurette-image img-responsive" src="./paper_figs/album_teaser2.png" alt="Generic placeholder image" style="width: 340px; height: 170px;">
        </div>
        <div class="col-sm-9">
          <h4>Learning to Select and Order Vacation Photographs </h4>
          <p style="color:DimGray">we propose the problem of automatic photo album creation from an unordered image collection. To help solve this problem, we collect a new benchmark dataset based on Flicker images. We analyze the problem and provide experimental evidence, through user studies, that both selection and ordering of photos within an album is important for human observers. To capture and learn rules of album composition, we propose a discriminative structured model capable of encoding simple prefer ences for contextual layout of the scene and ordering between photos. The parameters of the model are learned using a structured SVM framework.</p>
<p> <span style="color:DarkOrchid"><font size="4" > <b>Fereshteh Sadeghi</b> </span><span style="color:#2874A6"><font size="3" >, J. Rafael Tena, Ali Farhadi, Leonid Sigal </span></p> 
          <h5 style="color:DarkCyan">WACV 2015</h5>
          <p><a target="_blank" href="./papers/fsadeghi_album_wacv15.pdf" class="btn btn-xs btn-success btn" role="button">paper</a>
             <a target="_blank" href="#" class="btn btn-xs btn-primary btn" role="button">poster</a>
             <!--<a href="#" class="btn btn-xs btn-info btn" role="button">snapshot</a> -->
             <a class="btn btn-xs btn-info btn" href="https://www.disneyresearch.com/publication/learning-to-select-and-order-vacation-photographs/" role="button">Dataset</a></p>
        </div>
      </div>


 <hr class="featurette-divider">
      <div class="row featurette">
        <div class="col-sm-3">
          <p>     </p>
          <p>     </p>
          <img class="featurette-image img-responsive" src="./paper_figs/fsadeghi_decoding_tex_encoding.png" alt="Generic placeholder image" style="width: 340px; height: 170px;">
        </div>
        <div class="col-sm-9">
          <h4>Decoding the Text Encoding</h4>
          <p style="color:DimGray">Despite the attractiveness and simplicity of producing word clouds, they do not provide a thorough visualization for the distribution of the underlying data. Our proposed method is able to decode an input word cloud visualization and provides the raw data in the form of a list of (word, value) pairs. To the best of our knowledge our work is the first attempt to extract raw data from word cloud visualization. The results of our experiments show that our algorithm is able to extract the words and their weights effectively with considrerable low error rate.</p>
<p> <span style="color:DarkOrchid"><font size="4" > <b>Fereshteh Sadeghi</b> </span><span style="color:#2874A6"><font size="3" >,Hamid Izadinia </span></p>  
          
          <h5 style="color:DarkCyan">arXiv:1412.6079, 2014</h5>
          <p><a target="_blank" href="./papers/fsadeghi_decoding_text_encoding.pdf" class="btn btn-xs btn-success btn" role="button">paper</a>
        </div>
      </div>


      <hr class="featurette-divider">
      <div class="row featurette">
        <div class="col-md-3">
          <p>   </p>
          <img class="featurette-image img-responsive" src="./paper_figs/lpr_teaser.png" alt="Generic placeholder image" style="width: 340px; height: 170px;">
        </div>
        <div class="col-md-9">
          <h4>Latent pyramidal regions for recognizing scenes </h4>
          <p style="color:DimGray">In this paper we proposed a simple but efficient image representation for solving the scene classification problem. Our new representation combines the benefits of spatial pyramid representation using nonlinear feature coding and latent Support Vector Machine (LSVM) to train a set of Latent Pyramidal Regions (LPR). Each of our LPRs captures a discriminative characteristic of the scenes and is trained by searching over all possible sub-windows of the images in a latent SVM training procedure. The final response of the LPRs form a single feature vector which we call the LPR representation and can be used for the classification task.</p>
<p> <span style="color:DarkOrchid"><font size="4" > <b>Fereshteh Sadeghi</b> </span><span style="color:#2874A6"><font size="3" >, Marshall Tappen </span></p>          
          <h5 style="color:DarkCyan">ECCV 2012</h5>
          <p><a target="_blank" href="./papers/fsadeghi_LPR_eccv12.pdf" class="btn btn-xs btn-success btn" role="button">paper</a></p>
             <!--<a href="#" class="btn btn-xs btn-primary btn" role="button">poster</a></p> -->
        </div>
      </div>   
      <hr class="featurette-divider">
      <div class="row featurette">
        <div class="col-md-1">
          <p></p>
        </div>  
        <div class="col-md-2">
          <p>   </p>
          <img class="featurette-image img-responsive" src="./paper_figs/labelTree_img.jpg" alt="" style="width: 120px; height: 120px";>
        </div>
        <div class="col-md-9">
          <h4>Probabilistic Label Trees for Efficient Large Scale Image Classification </h4>
          <p style="color:DimGray"> Large-scale recognition problems with thousands of classes pose a particular challenge because applying the classifier requires more computation as the number of classes grows. The label tree model integrates classification with the traversal of the tree so that complexity grows logarithmically. We show how the parameters of the label tree can be found using maximum likelihood estimation. This new probabilistic learning technique produces a label tree with significantly improved recognition accuracy.</p>
<p> <span style="color:#2874A6"><font size="3" > Baoyuan Liu, </span><span style="color:DarkOrchid"><font size="4" > <b>Fereshteh Sadeghi</b> </span><span style="color:#2874A6"><font size="3" > , Marshall Tappen, Ohad Shamir, Ce Liu </span></p>
          <h5 style="color:DarkCyan">CVPR 2013</h5>
          <p><a target="_blank" href="./papers/fsadeghi-CVPR13.pdf" class="btn btn-xs btn-success btn" role="button">paper</a></p>
             <!--<a href="#" class="btn btn-xs btn-primary btn" role="button">poster</a></p> -->
        </div>
      </div>      
      

<h2>Teaching (GTA) </h2>
    <hr class="featurette-divider">
      <div class="row featurette">
        
        <div class="col-md-3">
          <p>   </p>
          <img class="featurette-image img-responsive" src="./paper_figs/pacman.png" alt="" style="width: 350px; height: 150px";>
        </div>
        <div class="col-md-9">
          <h4> Artificial Intelligence </h4>
          <ul>
            <li style="color:DarkOrchid "> <h5> <b> <a style="color:DarkOrchid" target="_blank" href="http://courses.cs.washington.edu/courses/csep573/">CSEP 573 - Winter 2016</a></b></h5></li>
            <li style="color:DarkOrchid "> <h5> <b> <a style="color:DarkOrchid" target="_blank" href="https://courses.cs.washington.edu/courses/cse473/">CSE 473 - Autumn 2015</a> </b></h5></li>
          </ul>
          
          <p style="color:DimGray"> Search, Expectimax, CSP, MDP, Reinforcement Learning, Q-Learning, Uncertainty, Hidden Markov Models (HMMs), Baysian Networks (BNs), Naive Bayes, Perceptron, and fun PacMan game. </p>
          </p>
             <!--<a href="#" class="btn btn-xs btn-primary btn" role="button">poster</a></p> -->
        </div>
      </div> 
<hr class="featurette-divider">

<footer class="bs-docs-footer" role="contentinfo">
  <p style="color:DimGray"><center>&copy; Fereshteh Sadeghi</p>
<p><a href="https://twitter.com/fereshteh_sa?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false">Follow @fereshteh_sa</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>
  <p><center>
    <a target="_blank" href="https://www.facebook.com/fereshteh.sadeghi"><img src="./paper_figs/fb.png" alt="Generic placeholder image" style="width: 25px; height: 25px;"></a>
    <a target="_blank" href="https://plus.google.com/u/1/112656789221450240805/posts"><img src="./paper_figs/button-gplus.png" alt="Generic placeholder image" style="width: 27px; height: 27px;"></a> 
    <a target="_blank" href="http://instagram.com/fereshs/"><img src="./paper_figs/instag.jpeg" alt="Generic placeholder image" style="width: 22px; height: 22px;"></a>
<a target="_blank" href="https://twitter.com/fereshteh_sa"><img src="./paper_figs/twitter.png" alt="Generic placeholder image" style="width: 30px; height: 30px;"></a> 
  </center></p>


</footer>
<script src="../codes/my.js"></script>

</body>
</html>
